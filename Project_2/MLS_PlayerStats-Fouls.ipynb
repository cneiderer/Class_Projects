{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping MLS Player Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping www.mlssoccer.com/ for player statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from dateutil import parser\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Scraper and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def send_request(url, params={}, attempts=3):\n",
    "\n",
    "    cnt = 0\n",
    "    while cnt <= attempts:\n",
    "        cnt += 1        \n",
    "        # Send Request\n",
    "        response = requests.get(url, params)\n",
    "        # Check Status\n",
    "        print(response.url, '\\n', response.status_code, response.reason)\n",
    "        if (response.status_code >= 200) and (response.status_code < 300):           \n",
    "            return response\n",
    "        # If bad status, pause before trying again\n",
    "        print('Pause, then retry')\n",
    "        time.sleep(15)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_id_to_club(franchise_id):\n",
    "    \n",
    "    franchise_map = {11091: 'ATL',\n",
    "                     1207: 'CHI',\n",
    "                     436: 'COL',\n",
    "                     454: 'CLB',\n",
    "                     1326: 'DC',\n",
    "                     1903: 'DAL',\n",
    "                     1897: 'HOU',\n",
    "                     1230: 'LA',\n",
    "                     11690: 'LAFC',\n",
    "                     6977: 'MNUFC',\n",
    "                     1616: 'MTL',\n",
    "                     928: 'NE',\n",
    "                     9668: 'NYCFC',\n",
    "                     399: 'NYRB',\n",
    "                     6900: 'ORL',\n",
    "                     5513: 'PHI',\n",
    "                     1581: 'POR',\n",
    "                     1899: 'RSL',\n",
    "                     1131: 'SJ',\n",
    "                     3500: 'SEA',\n",
    "                     421: 'KC',\n",
    "                     2077: 'TOR',\n",
    "                     1708: 'VAN'}\n",
    "    return franchise_map[franchise_id]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_for_data(stats_table):\n",
    "    # Check if the table has any \n",
    "    odd_rows = stats_table.findAll('tr', {'class': 'odd'})\n",
    "    if odd_rows[0].text.strip() == 'Stats Unavailable':\n",
    "        return False\n",
    "    return True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_player_name(df):\n",
    "    first = []\n",
    "    last = []\n",
    "    for t in df.Player.str.split():\n",
    "        if len(t) == 1:\n",
    "            first.append(t[0])\n",
    "            last.append('')\n",
    "        elif len(t) == 2:\n",
    "            first.append(t[0])\n",
    "            last.append(t[1])\n",
    "        else:\n",
    "            first.append(t[0])\n",
    "            last.append(' '.join(t[1:]))\n",
    "    \n",
    "    df['Last'] = last\n",
    "    df['First'] = first  \n",
    "    \n",
    "    df = df.drop('Player', axis=1)\n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Find and parse franchise popup menu to get list of teams\n",
    "# soup = BeautifulSoup(response.text, 'lxml')\n",
    "# franchise_select = soup.find('select', {'id': 'edit-franchise', 'name': 'franchise'})\n",
    "# franchise = []\n",
    "# # start with second index since first is generic \"Select A Club\"\n",
    "# for team in franchise_select.findAll('option')[1:]:\n",
    "#     franchise.append(int(team['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_param_combos(param_groups):\n",
    "    params = []\n",
    "    for combo in itertools.product(*param_groups):\n",
    "        params.append({'franchise': combo[0],\n",
    "                       'group': combo[1],\n",
    "                       'season_type': combo[2],\n",
    "                       'year': combo[3],\n",
    "                       'page': combo[4]})\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_player_stats(base_url, params, return_last_pg=False):\n",
    "\n",
    "    response = send_request(base_url, params)\n",
    "    if response is None:\n",
    "        return None\n",
    "    \n",
    "    # Parse HTML\n",
    "    soup = BeautifulSoup(response.text,'lxml')\n",
    "    \n",
    "    stats_table = soup.find('table')\n",
    "    \n",
    "    if not check_for_data(stats_table):\n",
    "        return None\n",
    "    \n",
    "    stats_df = extract_stats(stats_table)\n",
    "    # Add Year Column\n",
    "    stats_df['Year'] = np.repeat(int(params['year']), len(stats_df))\n",
    "    # Add Season Column\n",
    "    stats_df['Season'] = np.repeat(params['season_type'], len(stats_df))\n",
    "    \n",
    "    if return_last_pg:\n",
    "        return stats_df, get_last_page(soup)\n",
    "    \n",
    "    return stats_df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_stats(stat_table):\n",
    "    \n",
    "    # Extract salary data\n",
    "    stat_header = []\n",
    "    stat_data = []\n",
    "    for row in stat_table.findAll('tr'):\n",
    "        row_data = []\n",
    "\n",
    "        # Get row type and check if header or data row\n",
    "        row_type = row.findChild().name\n",
    "        if row_type == 'th':\n",
    "            # Extract header\n",
    "            for h in stat_table.findAll('th'):\n",
    "                stat_header.append(h.text) \n",
    "        else:\n",
    "            # Extract data\n",
    "            for data in row.findAll('td'):\n",
    "                row_data.append(data.text)\n",
    "            stat_data.append(row_data)    \n",
    "            \n",
    "    # Compile stat dataframe\n",
    "    stat_df = pd.DataFrame(stat_data, columns=stat_header)\n",
    "    \n",
    "    # Strip any whitespace from column names\n",
    "    stat_df.columns = stat_df.columns.str.strip()\n",
    "    \n",
    "    # Replace blanks with nans\n",
    "    stat_df = stat_df.replace('', np.nan)\n",
    "        \n",
    "    return stat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_last_page(soup):\n",
    "\n",
    "    last_pg_url = soup.find('li', {'class': 'pager-last last'}).find('a')['href']\n",
    "    last_pg = int(re.search('(?<=page=)\\d+', last_pg_url).group())\n",
    "    \n",
    "    return last_pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_group_stats(base_url, group, year=np.arange(2007, 2018), franchise='select', season_type='REG'):\n",
    "    \n",
    "    group_df = pd.DataFrame()\n",
    "    combos = generate_param_combos([[franchise], [group], [season_type], year, [0]])\n",
    "    for params in combos: \n",
    "        stats_df, last_pg = scrape_player_stats(base_url, params, True)\n",
    "        \n",
    "        if stats_df is None:\n",
    "            continue\n",
    "            \n",
    "        # Scrape first page of results\n",
    "        group_df = pd.concat([group_df, stats_df], axis=0)\n",
    "        # Add pause to prevent 429 status\n",
    "        print(datetime.now())\n",
    "        time.sleep(np.random.uniform(2, 5)) \n",
    "        \n",
    "        # Scrape next through last\n",
    "        for idx in range(1, last_pg+1):\n",
    "            params['page'] = idx\n",
    "            stats_df = scrape_player_stats(base_url, params)\n",
    "            if stats_df is None:\n",
    "                continue\n",
    "            group_df = pd.concat([group_df, stats_df], axis=0)            \n",
    "            # Add pause to prevent 429 status\n",
    "            print(datetime.now())\n",
    "            time.sleep(np.random.uniform(2, 5)) \n",
    "            \n",
    "    # Reset index to remove duplicates created during concatenation\n",
    "    group_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return group_df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Send Request to Base URL and Verify Site is Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 'https://www.mlssoccer.com/stats/season')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'http://www.mlssoccer.com/stats/season'\n",
    "response = requests.get(base_url)\n",
    "response.status_code, response.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define parameter options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group = ['goals', 'assists', 'shots', 'fouls', 'goalkeeping']\n",
    "# for grp in group:\n",
    "#     print('Scraping:', grp, datetime.now(), '\\n')\n",
    "#     df = scrape_group_stats(base_url, grp, np.arange(2007, 2018, 1))\n",
    "#     df.to_pickle(grp + '_df.pkl')\n",
    "#     print('Completed Scraping:', grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.mlssoccer.com/stats/season?franchise=select&group=fouls&season_type=REG&year=2007&page=0 \n",
      " 200 OK\n",
      "2018-01-26 17:36:43.348369\n",
      "https://www.mlssoccer.com/stats/season?franchise=select&group=fouls&season_type=REG&year=2007&page=1 \n",
      " 200 OK\n",
      "2018-01-26 17:36:53.232375\n",
      "https://www.mlssoccer.com/stats/season?franchise=select&group=fouls&season_type=REG&year=2007&page=2 \n",
      " 200 OK\n",
      "2018-01-26 17:37:04.300632\n",
      "https://www.mlssoccer.com/stats/season?franchise=select&group=fouls&season_type=REG&year=2007&page=3 \n",
      " 200 OK\n",
      "2018-01-26 17:37:18.309698\n",
      "https://www.mlssoccer.com/stats/season?franchise=select&group=fouls&season_type=REG&year=2007&page=4 \n",
      " 200 OK\n",
      "2018-01-26 17:37:30.313162\n"
     ]
    }
   ],
   "source": [
    "fouls_df = scrape_group_stats(base_url, 'fouls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fouls_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fouls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fouls_df.to_pickle('fouls_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
